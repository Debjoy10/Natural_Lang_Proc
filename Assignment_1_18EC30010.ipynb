{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fk0evCZ7U9WO"
   },
   "source": [
    "# **Assignment 1 on Natural Language Processing**\n",
    "\n",
    "### Date : 4th Sept, 2020\n",
    "\n",
    "#### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il_b_LFKXi8t"
   },
   "source": [
    " # NLTK Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ss5CZjC2Xt0i"
   },
   "source": [
    "The [NLTK](https://www.nltk.org/) Python framework is generally used as an education and research tool. Tokenization, Stemming, Lemmatization, Punctuation, Character count, word count are some of these packages which will be discussed in this tutorial.\n",
    "\n",
    "**Installing Nltk** <br>\n",
    "Nltk can be installed using PIP or Conda package managers.For detailed installation instructions follow this [link](https://www.nltk.org/install.html).\n",
    "\n",
    "To ensure we are all on the same page, the coding environment will be in **python3**. We suggest downloading Anaconda3 and creating a separate environment to do this assignment. \n",
    "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. \n",
    "The steps to install NLTK is available on the link: \n",
    "```bash\n",
    "sudo pip3 install nltk \n",
    "python3 \n",
    "nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4txbU5-RlMv"
   },
   "source": [
    "**Note for Question and answers:**\n",
    "\n",
    "Write your answers to the point in the text box below labelled as **Answer here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52_aJRSqaHgC"
   },
   "source": [
    "# Tokenizing words and Sentences using Nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5_ElYaeaMbR"
   },
   "source": [
    "**Tokenization** is the process by which big quantity of text is divided into smaller parts called tokens. <br>It is crucial to understand the pattern in the text in order to perform various NLP tasks.These tokens are very useful for finding such patterns.<br>\n",
    "\n",
    "Natural Language toolkit has very important module tokenize which further comprises of sub-modules\n",
    "\n",
    "1. word tokenize\n",
    "2. sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sby_OS3qZ_fz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/debjoy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to /home/debjoy/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "import nltk\n",
    "nltk.download('punkt') # For tokenizers\n",
    "nltk.download('inaugural') # For dataset\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ew9Aq5WHXSn-"
   },
   "outputs": [],
   "source": [
    "# Sample corpus.\n",
    "from nltk.corpus import inaugural\n",
    "corpus = inaugural.raw('1789-Washington.txt')\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2wbXKzVW0GO"
   },
   "source": [
    "### **TASK**:\n",
    "\n",
    "For the given corpus, \n",
    "1. Print the number of sentences and tokens. \n",
    "2. Print the average number of tokens per sentence.\n",
    "3. Print the number of unique tokens\n",
    "4. Print the number of tokens after stopword removal using the stopwords from nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrtu9HcHXFe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 23\n",
      "Number of tokens: 1537\n",
      "Average number of tokens per sentence: 66.83\n",
      "Number of unique tokens: 626\n",
      "Number of tokens after stopword removal: 800\n",
      "Number of unique tokens after stopword removal: 543\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentences = sent_tokenize(corpus)\n",
    "tokens = word_tokenize(corpus)\n",
    "sw = stopwords.words('english')\n",
    "tokens_without_sws = [t for t in tokens if t not in sw]\n",
    "\n",
    "print(\"Number of sentences: {}\".format(len(sentences)))\n",
    "print(\"Number of tokens: {}\".format(len(tokens)))\n",
    "print(\"Average number of tokens per sentence: {:0.2f}\".format(len(tokens)/len(sentences)))\n",
    "print(\"Number of unique tokens: {}\".format(len(set(tokens))))\n",
    "print(\"Number of tokens after stopword removal: {}\".format(len(tokens_without_sws)))\n",
    "print(\"Number of unique tokens after stopword removal: {}\".format(len(set(tokens_without_sws))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UViYY9_3t2UE"
   },
   "source": [
    "# Stemming and Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g55XX9KDLgO7"
   },
   "source": [
    "**What is Stemming?** <br>\n",
    "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.<br>\n",
    "Hence Stemming is a way to find the root word from any variations of respective word\n",
    "\n",
    "There are many stemmers provided by Nltk like **PorterStemmer**, **SnowballStemmer**, **LancasterStemmer**.<br>\n",
    "\n",
    "We will try and see differences between Porterstemmer and Snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SS4Ij__XLfTB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmed: ['grow', 'leav', 'fairli', 'cat', 'troubl', 'misunderstand', 'friendship', 'easili', 'ration', 'relat']\n",
      "SnowballStemmed: ['grow', 'leav', 'fair', 'cat', 'troubl', 'misunderstand', 'friendship', 'easili', 'ration', 'relat']\n",
      "\n",
      "Stemmed Corpus(Partially Displayed) - \n",
      "fellow-citizen of the senat and of the hous of repres: among the vicissitud incid to life no event could have fill me with greater anxieti than that of which the notif wa transmit by your order, and receiv on the 14th day of the present month . On the one hand, I wa summon by my countri, whose voic I can never hear but with vener and love, from a retreat which I had chosen with the fondest predilect, and, in my flatter hope, with an immut decis, as the asylum of my declin year--a retreat which wa render everi day more necessari as well as more dear to me by the addit of habit to inclin, and of frequent interrupt in my health to the gradual wast commit on it by time . On the other hand, the magnitud and difficulti of the trust to which the voic of my countri call me, be suffici to awaken in the wisest and most experienc of her citizen a distrust scrutini into hi qualif, could not but overwhelm with despond one who (inherit inferior endow from natur and unpract in the duti of civil admin ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer # Note that SnowballStemmer has language as parameter.\n",
    "\n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"misunderstanding\",\"friendships\",\"easily\", \"rational\", \"relational\"]\n",
    "\n",
    "# TODO\n",
    "# create an instance of both the stemmers and perform stemming on above words\n",
    "ps = PorterStemmer()\n",
    "ss = SnowballStemmer(language = \"english\")\n",
    "ps_words = [ps.stem(w) for w in words]\n",
    "ss_words = [ss.stem(w) for w in words]\n",
    "print(\"PorterStemmed: {}\".format(ps_words))\n",
    "print(\"SnowballStemmed: {}\".format(ss_words))\n",
    "\n",
    "# TODO\n",
    "# Complete the function which takes a sentence/corpus and gets its stemmed version.\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "def stemSentence(sentence=None):\n",
    "    # Generate tokens and stem them (Used PorterStemmer)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    ps_words = [ps.stem(w) for w in tokens]\n",
    "    \n",
    "    # Detokenize the stemmed tokens\n",
    "    stemmed_sentence = TreebankWordDetokenizer().detokenize(ps_words)\n",
    "    return stemmed_sentence\n",
    "\n",
    "print(\"\\nStemmed Corpus(Partially Displayed) - \")\n",
    "print(stemSentence(corpus)[:1000] + \" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JuE8CuDQSno"
   },
   "source": [
    "**What is Lemmatization?** <br>\n",
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma.<br>\n",
    "\n",
    "*The NLTK Lemmatization method is based on WorldNet's built-in morph function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noyl1YNsQp98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/debjoy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['grows', 'leaf', 'fairly', 'cat', 'trouble', 'running', 'friendship', 'easily', 'wa', 'relational', 'ha']\n",
      "Lemmatized Words with POS=Noun: ['grows', 'leaf', 'fairly', 'cat', 'trouble', 'running', 'friendship', 'easily', 'wa', 'relational', 'ha']\n",
      "Lemmatized Words with POS=Verb: ['grow', 'leave', 'fairly', 'cat', 'trouble', 'run', 'friendships', 'easily', 'be', 'relational', 'have']\n",
      "\n",
      "Example:\n",
      "Difference in lemmatization with POS for 'leaves': leaf(with POS=Noun), leave(with POS=Verb)\n",
      "\n",
      "Lemmatized Corpus(Partially Displayed) - \n",
      "Fellow-Citizens of the Senate and of the House of Representatives: Among the vicissitude incident to life no event could have filled me with greater anxiety than that of which the notification wa transmitted by your order, and received on the 14th day of the present month . On the one hand, I wa summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hope, with an immutable decision, a the asylum of my declining year--a retreat which wa rendered every day more necessary a well a more dear to me by the addition of habit to inclination, and of frequent interruption in my health to the gradual waste committed on it by time . On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizen a distrustful scrutiny into his qualification, could not but overwhelm wit ...\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') # Since Lemmatization method is based on WorldNet's built-in morph function.\n",
    "\n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"running\",\"friendships\",\"easily\", \"was\", \"relational\",\"has\"]\n",
    "\n",
    "#TODO\n",
    "# Create an instance of the Lemmatizer and perform Lemmatization on above words\n",
    "# You can also give Parts-of-speech(pos) to the Lemmatizer for example \"v\" (verb). Check the differences in the outputs.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatized_words = [lemmatizer.lemmatize(w) for w in words]\n",
    "n_lemmatized_words = [lemmatizer.lemmatize(w, pos=\"n\") for w in words]\n",
    "v_lemmatized_words = [lemmatizer.lemmatize(w, pos=\"v\") for w in words]\n",
    "\n",
    "print(\"Lemmatized Words: {}\".format(lemmatized_words))\n",
    "print(\"Lemmatized Words with POS=Noun: {}\".format(n_lemmatized_words))\n",
    "print(\"Lemmatized Words with POS=Verb: {}\".format(v_lemmatized_words))\n",
    "\n",
    "# The word leaves can be lemmatized to either leaf/leave depending upon the POS tag(Noun/Verb)\n",
    "ln = lemmatizer.lemmatize(\"leaves\", pos=\"n\")\n",
    "lv = lemmatizer.lemmatize(\"leaves\", pos=\"v\")\n",
    "print(\"\\nExample:\\nDifference in lemmatization with POS for 'leaves': {}(with POS=Noun), {}(with POS=Verb)\".format(ln, lv))\n",
    "\n",
    "#TODO\n",
    "# Complete the function which takes a sentence/corpus and gets its lemmatized version.\n",
    "def lemmatizeSentence(sentence=None):\n",
    "    # Generate tokens and lemmatize them\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lm_words = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    # Detokenize the lemmatize tokens\n",
    "    lemmatized_sentence = TreebankWordDetokenizer().detokenize(lm_words)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "print(\"\\nLemmatized Corpus(Partially Displayed) - \")\n",
    "print(lemmatizeSentence(corpus)[:1000] + \" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJW6HsycSAlU"
   },
   "source": [
    "**Question:** Give example of two words which have same stem but different lemma? Show the stem and lemma of both words in the code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcq6bUEaSAt1"
   },
   "source": [
    "**Answer here:** Two suitable words can be \"leaving\" and \"leaves\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8OtIEmFkGBM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words:   leaving, leaves\n",
      "Lemmatized words: leaving, leaves\n",
      "Stemmed words:    leav, leav\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# Write code to print the stem and lemma of both your words\n",
    "w1 = \"leaving\"\n",
    "w2 = \"leaves\"\n",
    "\n",
    "lmw1 = lemmatizer.lemmatize(w1)\n",
    "lmw2 = lemmatizer.lemmatize(w2, pos=\"a\")\n",
    "psw1 = ps.stem(w1)\n",
    "psw2 = ps.stem(w2)\n",
    "\n",
    "print(\"Original words:   {}, {}\".format(w1, w2))\n",
    "print(\"Lemmatized words: {}, {}\".format(lmw1, lmw2))\n",
    "print(\"Stemmed words:    {}, {}\".format(psw1, psw2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0tz3SIGSA2b"
   },
   "source": [
    "**Question:** Write a comparison between stemming and lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aks8QaQ_SA_N"
   },
   "source": [
    "**Answer here:** Stemming process only consists of rule-based algorithms for removal of prefix/suffux of a word and it doesn't take into account the word morphology or the tense of the word in the text. Lemmatization makes use of morphological analysis and a word vocabulary for removing suffix/prefix of words and tries to conserve the meaning of the word. (Similar to how \"leaving\" and \"leaves\" both were stemmed to \"leav\", which is a shorter version but not an actual word, while lemmatization retains the original words to conserve the sematic correctness.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Assignment 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
